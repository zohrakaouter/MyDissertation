\chapter{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}
\chaptermark{Conclusion}
This chapter summarizes the contributions (Chapter~\ref{autocoevol},~\ref{chapitre2} and~\ref{chapitre3}) of this dissertation.% highlighting their results.
  Each contribution is briefly revisited to highlight its results in addressing the research objectives and its limitations. Additionally, this chapter explores potential future directions and perspectives for each contribution.%, outlining opportunities for further advancements and improvements in this domain. Through these reflections, this work aims to provide a foundation for continued research and innovation.



\section{ Summary of contributions and limitations}
%\subsection{Automated testing of metamodels and code co-evolution}
%\subsection{An Empirical Study on Leveraging LLMs for Metamodels and Code Co-evolution}
In this thesis, I started by proposing an automatic code co-evolution approach with evolving metamodel.
This approach effectively automates code co-evolution, resolving~100\% of errors, that we can divide into direct and indirect errors, and reaching 82\% of precision and 81\% of recall. The automation of the approach allows significantly reducing manual effort. The strategy prioritizes minimal deletions when possible, ensuring that necessary changes do not remove more code than required to apply the metamodel changes. Moreover, test execution results showed no significant behavioral differences between the original and co-evolved code, reinforcing the correctness of the automated approach in preserving code functionality. Additionally, the method is computationally efficient, with an average resolution time of less than half a second per error, making it scalable even on standard hardware. The comparison with IDE quick fixes highlights their limitations in handling co-evolution scenarios. Quick fixes omit contextually information of  metamodel changes, leading to applying different fixes than the expected developers' resolutions.
Finally, the comparison with the semi-automatic co-evolution approach \cite{Khelladi2020} highlights a key trade-off between automation and precision with a difference of almost -10\%, knowing that the automatic approach logs the detailed co-evolution operations in a report that developers can check later. 

%limitations

%- I did not handle multiple, inter-dependent metamodels' evolution.
%- 
%- Evaluation does not cover of poThe size of the projects, number resulted compiling errors is among limitations.
%- I evaluated only on EMF data set only and no other technological space.ssible situations of metamodel changes and errors.
%-I use Evosuite generated tests only and not manually written tests.

While our approach demonstrates strong performance in automating code co-evolution, it has several limitations. First, it does not handle cases where multiple, interdependent metamodels evolve simultaneously, which can introduce additional complexity. Additionally, the evaluation is constrained by the size of the projects and the number of resulting compilation errors, limiting the generalizability of the results. Another limitation is the sole focus on EMF dataset, without an assessment its applicability in other technological spaces. Furthermore, the evaluation does not cover all possible metamodel changes and error scenarios, meaning certain edge cases may not be addressed. Lastly, our testing relies only on EvoSuite-generated tests, without incorporating manually written tests, that we argued its usefulness, but may not be complete in the context of out evaluation.



%My thorough evaluation proved that automatic co-evolution gives good results comparing to semi-automatic code co-evolution
Second, I developed an approach for automated testing of code co-evolution. 
Before tackling the evaluation of the approach itself, I conducted a user study to highlight the challenges of manually tracing impacted tests after metamodel evolution which can be both incomplete and error-prone. Since the approach automated testing of code co-evolution is based on tracing the impacted set of tests before and after co-evolution, I had to confirm that the approach successfully traces impacted tests due to metamodel changes, isolating the relevant tests. At this point of the evaluation, I included projects with both manually written and automatically generated tests. The correctness of the co-evolution is indicated by the fluctuation in the number of passing, failing, or erroneous tests. Overall, I found that running the traced subset of tests can indicate if the co-evolution was processed correctly or not, regardless of whether they tests are manually or automatically written, thus gaining effort and time.

%Limitation
%- The size of the projects, number resulted compiling errors is among limitations.
%- I evaluated only on EMF data set only and no other technological space.
%- The visual output of our tool is still basic, and has to be improved in order to assist developers in the code co-evolution process.
While our approach shows promising results, it has several limitations. The size of the projects and the number of both manually written and generated tests present challenges, as larger projects with extensive test suites may require additional optimization for efficient handling. Additionally, our evaluation was conducted only on EMF dataset, limiting its applicability to other technological spaces. Future work should explore its effectiveness in different contexts. Moreover, the visual output of our tool remains basic and needs improvement to better assist developers in the code co-evolution process by providing clearer insights and more intuitive interactions.

Finally, with the emergence of Large Language Models, I Investigated in my empirical study the ability of Chatgpt to give correct co-evolutions. Setting the hyperparameter temperature to 0.2, I obtained  88.7\% of correctness rate in handling code co-evolution, improving to 95.2\% for complex metamodel changes, thus highlighting ChatGPT’s promising potential. Then, I analyzed the impact of varying the temperature on the results. I found that lower temperature of 0 and 0.2 give better coevolutions. Interestingly, we obtained the same results over 5 different runs, which suggests the efficiency our
prompt structure in narrowing the scope of possibilities of ChatGPT’s answers. Furthermore, I studied the impact of varying the prompt structure of the prompts on the quality of Chatgpt proposed co-evolutions. Providing only the code instruction containing the error yielded the best results, in some cases, as it helped ChatGPT focus on the specific issue. Asking for alternative answers also improved accuracy but to a lesser extent. However, changing the order by describing the metamodel change first instead of the describing the abstraction gap between the metamodel impacted element and the erroneous code led to a drop in correctness, as ChatGPT sometimes assumed deleted elements still existed. Thus, showing that well-structured prompts significantly enhance ChatGPT’s performance. Finally, the comparison of the proposed co-evolutions with quick fixes revealed significant limitations in the latter. The lack of context-awareness in IDE quick fixes and understanding of the abstraction gap made them unsuitable for the co-evolution task. In contrast, our prompt generation approach addresses these gaps by considering the impacted code’s context and the metamodel changes, guided Chatgpt to propose more accurate co-evolutions.

%Limitations 
%- The empirical study was based only gpt-3.5, no other large language models was used to get more confidence on the results.
%- The LLM hyper parameters: top\_p was fixed, the temperature was variated. Studying the variation of all hyperparameters may give more insights on the ability of the model in proposing co-evolution
%- The empirical study was base don a prototype, the is not a yet a tool to assist developers in code co-evolution, that is based on our approach.
Regarding limitations of this study include the fact that the empirical evaluation was conducted only on GPT-3.5, without testing other Large Language Models, which could provide greater confidence in the results. Additionally, among the LLM hyperparameters, only the temperature was varied while top\_p remained fixed. A more comprehensive analysis involving the variation of all hyperparameters could offer deeper insights into the model's capability in proposing co-evolutions. Furthermore, the study was based on a prototype implementation, and there is not yet a fully developed tool to assist developers in code co-evolution based on our approach.
\section{Perspectives}
In light of the findings presented in this thesis, there are a number of perspectives that can guide our future research. In this section, we suggest some potential future research topics concerning co-evolution problem.

\subsection{Automated co-evolution of metamodels and code}

We first plan to take into consideration the case of simultaneous multiple metamodel evolution. 
Then, we are interested in exploring different ordering of the errors in the code before co-evolving them. In fact, we co-evolve the errors in the same order they are retrieved by the Java Develomplent Tools (JDT). Thus, we will investigate whether it could reach better correctness with faster co-evolution or not. 
%Finally, we plan to extend our resolutions with a replace resolution, based on distance metrics to find element replacements to co-evolve the code. 
%After that, 
This can be done relying on a search-based heuristics, such as genetic algorithms to explore different paths of co-evolution, in contrast to a single path that we compute in Chapter~\ref{autocoevol}. In particular, to explore the different alternative resolutions for the deletion changes. A possible application of such algorithms is the use of NSGA-II. Given a set of compilation errors, the population consists of candidate solutions, where each solution is a sequence of resolutions. The optimization process aims to minimize the number of compilation errors while maximizing the number of passing tests as objective functions.

We also plan to evaluate our approach on more case studies. One particularly interesting technological space is JHipster, a low-code ecosystem with a workflow similar to EMF. JHipster starts with an abstract JHipster-specific Domain Language (JDL), which serves as a domain model used to generate various applications, such as front-end apps, back-end apps, and database schemas. As a first step, we aim to investigate the impact of JDL model evolution on its dependent applications and then apply our co-evolution approach to mitigate breaking changes.

As a global view for code co-evolution problem, we are curious to study it including the unit tests' code. In the MDE ecosystem, when the metamodel evolves, that impacts the code including the tests because these ones also use the elements of the metamodel. The core principle of co-evolution remains unchanged, but additional effort is needed for validation of the tests' co-evolution.
%TODO develop?
%


%
%a\kaouter{}
%\DK{either we put it as future work, maybe even in discussion above 4.5, and then refer to it here and say future work.}
%
%Finally, we will evaluate our approach on more case studies and on the case of simultaneous multiple metamodel evolution. 

%\newpage

\subsection{Automated testing of metamodels and code co-evolution}
First, we plan to improve the performance of our implementation with optimization of the tests' tracing.  %first evaluate on more case studies that could have manually written test. 
%we will 
We also plan to extend our approach to projects that use an equivalent form of metamodels in other technological space than Eclipse, such as JHipster and OpenAPI that both generate code from a model specification similar to a metamodel. Thus, we can have alternative case studies. 

Moreover, as presented in Section~\ref{coevolutioncode},client code co-evolution with evolving API related approaches, do not use any mechanism to check if the co-evolution impacted or not the behavioral correctness of the code. We are interested in replicating one of these approaches and apply our test tracing approach to check the correctness of its co-evolution.

After that, we plan to investigate the techniques of test amplification on the selected tests we traced from the metamodel changes. Indeed, once we select a subset of tests, we could amplify them by generating more similar tests, yet, with different assertions to cover more corner cases. This would amplify the behavioral check of the code co-evolution. 

Furthermore, the knowledge about the generated code elements from the metamodel elements (e.g., getter/setter for EAttribute, class/interface for EClass, etc.) is so far hard-coded in the implementation. The mappings must be provided for our approach to be able to trace the tests. We could express these mappings in a generic way as a configuration file taken as input in our approach. Thus, we could reuse our approach more easily in other scenarios than EMF Eclipse (e.g., jHipster), given their corresponding mappings as input. 
%This is left as future work to generalize our approach.

Finally, we will also explore another type of amplification, which is the interchange of the tests between the original and evolved versions. In other words, we aim to co-evolve the tests of the original and evolved versions, respectively forward and backward to the evolved and original versions, while removing duplicates. 

 

%%


%evaluate existing approaches of automatic code co-evolution with out technique to better assess their effect on the code. 

\subsection{An Empirical Study on Leveraging LLMs for Metamodels and Code Co-evolution}


First, we intend to evaluate or approach in other technological spaces than EMF, such as OpenAPI and the challenge of the API evolution impact on clients' code. After that, we plan to transform our approach into a DSL-based approach with a graphical user interface for the output report instead of a CSV file. To facilitate prompt generation and enhance the option of prompt variation, a DSL would be a viable solution. We also plan to replicate our empirical study with other LLMs and other contexts of co-evolution (e.g., between code and test \cite{le2021untangling}). Another actionable element is to investigate the mining of contextual information from Software Engineering tasks to enrich the prompts, and then improve baseline results.

Unlike our automatic co-evolution approach detailed in Chapter~\ref{autocoevol}, the use of LLMs for the co-evolution of other artifacts in MDE ecosystem is still unexploited. Thus, we initially aim to revisit the empirical study on leveraging LLMq for metamodel co-evolution with models, transformations, and constraints. The future results of these empirical studies may guide us in providing practical tools for MDE community. 


Finally, since our contributions focus on empirically studying the use of \LLM~in metamodel and code co-evolution, we plan to implement an alternative of the quick fix engine in the Eclipse IDE based on our generated prompt structure. Integrating our prompt-based metamodel and code co-evolution in the IDE will have a direct impact in helping MDE developers and language engineers. 
