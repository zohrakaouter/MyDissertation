\chapter{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}
\chaptermark{Conclusion}
This chapter summarizes the contributions (Chapters~\ref{autocoevol},~\ref{chapitre2},~and~\ref{chapitre3}) of this dissertation.~Each contribution is briefly revisited to highlight its results in addressing the research objectives, and its limitations. Additionally, this chapter explores potential future directions and perspectives for each contribution.%, outlining opportunities for further advancements and improvements in this domain. Through these reflections, this work aims to provide a foundation for continued research and innovation.



\section{ Summary of contributions and limitations}
\label{Summaryandlimitations}
%\subsection{Automated testing of metamodels and code co-evolution}
%\subsection{An Empirical Study on Leveraging LLMs for Metamodels and Code Co-evolution}
In this thesis, I started by proposing an automatic code co-evolution approach with evolving metamodel. It is a pattern-matching base approach between metamodel changes and code errors to select automatically their resolutions.
This approach effectively automates code co-evolution, resolving~100\% of errors, that can be divided into direct and indirect errors, and reaching 82\% of precision and 81\% of recall. The automation of the approach allows significantly reducing manual effort. The strategy prioritizes minimal deletions when possible, ensuring that necessary changes do not remove more code than required to apply the metamodel changes. Moreover, test execution results showed no significant behavioral differences between the original and co-evolved code, reinforcing the correctness of the automated approach in preserving code behavior. Additionally, the method is computationally efficient, with an average resolution time of less than half a second per error, making it scalable even on standard hardware. The comparison with IDE quick fixes highlights their limitations in handling co-evolution scenarios. Quick fixes omit contextually information of metamodel changes, leading to applying different fixes than the expected developers' resolutions.
Finally, the comparison with the semi-automatic co-evolution approach \cite{Khelladi2020} highlights a key trade-off between automation and precision with a difference of almost -10\%, knowing that the automatic approach logs the detailed co-evolution operations in a report that developers can check later. 

%limitations

%- I did not handle multiple, inter-dependent metamodels' evolution.
%- 
%- Evaluation does not cover of poThe size of the projects, number resulted compiling errors is among limitations.
%- I evaluated only on EMF data set only and no other technological space.ssible situations of metamodel changes and errors.
%-I use Evosuite generated tests only and not manually written tests.

While my approach demonstrates strong performance in automating code co-evolution, it has several limitations. First, it does not handle cases where multiple, interdependent metamodels evolve simultaneously, which can introduce additional complexity. Additionally, the evaluation is constrained by the size of the projects and the number of resulting compilation errors, limiting the generalizability of the results. Another limitation is the sole focus on EMF dataset, without an assessment of its applicability in other technological spaces. Furthermore, the evaluation does not cover all possible metamodel changes and error scenarios, meaning certain edge cases may not be addressed. Lastly, my testing relies only on EvoSuite-generated tests, without incorporating manually written tests, that I argued its usefulness, but may not be complete in the context of the approach's evaluation.



%My thorough evaluation proved that automatic co-evolution gives good results comparing to semi-automatic code co-evolution
Second, I developed an approach for automated testing of code co-evolution. This approach traces the tests before and after co-evolution starting from metamodel elements that are changed to the code.
Before tackling the evaluation of the approach itself, I conducted a user study to highlight the challenges of manually tracing impacted tests after metamodel evolution which can be both incomplete and error-prone. Since the approach of automated testing of code co-evolution is based on tracing the impacted set of tests before and after co-evolution, I had to confirm that the approach successfully traces impacted tests due to metamodel changes, isolating the relevant tests. At this point of the evaluation, I included projects with both manually written and automatically generated tests. The correctness of the co-evolution is indicated by the fluctuation in the number of passing, failing, and erroneous tests. Overall, I found that running the traced subset of tests can indicate if the co-evolution was processed correctly or not, regardless of whether they tests are manually written or automatically generated . Relatively to the evaluation, this allowed me to have a reduction of 88\% in the number of tests and 84\% in execution time.

%Limitation
%- The size of the projects, number resulted compiling errors is among limitations.
%- I evaluated only on EMF data set only and no other technological space.
%- The visual output of our tool is still basic, and has to be improved in order to assist developers in the code co-evolution process.
Nevertheless, there are some points that need further improvement. The size of the projects and the number of both manually written and generated tests present challenges, as larger projects with extensive test suites may require additional optimization for efficient handling. Additionally, the evaluation was conducted only on EMF dataset, limiting its applicability to other technological spaces. Future work should explore its effectiveness in different contexts. Moreover, the visual output of my tool remains basic and needs improvement to better assist developers in the code co-evolution process by providing clearer insights and more intuitive interactions.

Regarding my third and last contribution, with the emergence of Large Language Models, I investigated in my empirical study the ability of Chatgpt to give correct co-evolutions for metamodel and code. Setting the temperature hyperparameter to~0.2, I obtained~88.7\% of correctness rate in handling code co-evolution, improving to~95.2\% for complex metamodel changes, thus highlighting ChatGPT’s promising potential.~Then,~I analyzed the impact of varying the temperature on the results. I found that lower temperature of~0 and~0.2 give better co-evolutions. Interestingly, I obtained the same results over~5 different runs, which suggests the efficiency my
prompt structure in narrowing the scope of possibilities of ChatGPT’s answers. Furthermore, I studied the impact of varying the prompt structure of the prompts on the quality of Chatgpt proposed co-evolutions. Providing only the code instruction containing the error yielded the best results, in some cases, as it helped ChatGPT focus on the specific issue. Asking for alternative answers also improved accuracy but to a lesser extent. However, changing the order by describing the metamodel change first instead of the describing the abstraction gap between the metamodel impacted element and the erroneous code led to a drop in correctness, as ChatGPT sometimes assumed deleted elements still existed. Thus, showing that well-structured prompts significantly enhance ChatGPT’s performance. Finally, the comparison of the proposed co-evolutions with quick fixes revealed significant limitations in the latter. The lack of context-awareness in IDE quick fixes and understanding of the abstraction gap made them unsuitable for the co-evolution task. In contrast, my prompt generation approach addresses these gaps by considering the impacted code’s context and the metamodel changes, guided Chatgpt to propose more accurate co-evolutions.

%Limitations 
%- The empirical study was based only gpt-3.5, no other large language models was used to get more confidence on the results.
%- The LLM hyper parameters: top\_p was fixed, the temperature was variated. Studying the variation of all hyperparameters may give more insights on the ability of the model in proposing co-evolution
%- The empirical study was base don a prototype, the is not a yet a tool to assist developers in code co-evolution, that is based on our approach.
However, there are still limitations that need to be addressed. One limitation is that the empirical evaluation was conducted only on GPT-3.5, without testing other Large Language Models, which could provide greater confidence in the results. Additionally, among the LLM hyperparameters, only the temperature was varied while top\_p remained fixed. A more comprehensive analysis involving the variation of all hyperparameters could offer deeper insights into the model's capability in proposing co-evolutions. Furthermore, the study was based on a prototype implementation, and there is not yet a fully developed tool to assist developers in code co-evolution based on my approach.
\section{Perspectives}
In light of the findings presented in this thesis, there are a number of perspectives that can guide my future research. In this section, I suggest some potential future research topics concerning co-evolution problem.

\subsection{Automated co-evolution of metamodels and code}

As an improvement to cover more cases, I could start by taking into consideration the case of simultaneous multiple metamodel evolution. 
Then, I am interested in exploring different ordering of the errors in the code before co-evolving them. In fact, I co-evolve the errors in the same order they are retrieved by the Java Development Tools (JDT). Thus, One promising direction is to investigate whether it could reach better correctness with faster co-evolution or not. 
%Finally, we plan to extend our resolutions with a replace resolution, based on distance metrics to find element replacements to co-evolve the code. 
%After that, 
This can be done relying on a search-based heuristics, such as genetic algorithms to explore different paths of co-evolution, in contrast to a single path that I compute in Chapter~\ref{autocoevol}. In particular, this can be done to explore the different alternative resolutions for the deletion changes. A possible application of such algorithms is the use of NSGA-II.~Given a set of compilation errors, the population consists of candidate solutions, where each solution is a sequence of resolutions. The optimization process aims to minimize the number of compilation errors while maximizing the number of passing tests as objective functions.

While this study provides valuable insights on EMF case study, further research is needed on more case studies to generalize the findings of Chapter~\ref{autocoevol}. One particularly interesting technological space is JHipster, a low-code ecosystem with a workflow similar to EMF. JHipster starts with an abstract JHipster-specific Domain Language (JDL), which serves as a domain model used to generate various applications, such as front-end apps, back-end apps, and database schemas. As a first step, I aim to investigate the impact of JDL model evolution on its dependent applications. I started this investigation at the end of this thesis, however, the work is still in its early stages. After that, I could apply my co-evolution approach to mitigate the breaking changes of JDL model on the code.

As a global view for code co-evolution problem, I am curious to study it including the unit tests' code. In the MDE ecosystem, when the metamodel evolves, that impacts the code including the tests because these ones also use the elements of the metamodel. The core principle of co-evolution remains unchanged, but additional effort is needed for the validation of the tests' co-evolution.
%TODO develop?
%


%
%a\kaouter{}
%\DK{either we put it as future work, maybe even in discussion above 4.5, and then refer to it here and say future work.}
%
%Finally, we will evaluate our approach on more case studies and on the case of simultaneous multiple metamodel evolution. 

%\newpage

\subsection{Automated testing of metamodels and code co-evolution}
First, given the required computation of the tracing approach to analyze the code, it would be beneficial to improve the performance of my implementation alongside the optimization of the tests' tracing algorithm.  %first evaluate on more case studies that could have manually written test. 
%we will 
Expanding this approach to other projects that use an equivalent form of metamodels in other technological space than Eclipse could yield new insights. As potential case studies, there are JHipster and OpenAPI that both generate code from a model specification similar to a metamodel.% Thus, I can have alternative case studies. 

Moreover, as presented in Section~\ref{coevolutioncode}, client code co-evolution with evolving API related approaches, do not use any mechanism to check if the co-evolution impacts or not the behavioral correctness of the code. I am interested in replicating one of these approaches and apply my test tracing approach to check the correctness of its co-evolution.

After that, I could investigate the techniques of test amplification on the selected tests that I traced from the metamodel changes. Indeed, once I select a subset of tests, I could amplify them by generating more similar tests, yet, with different assertions to cover more corner cases. This would amplify the behavioral check of the code co-evolution. 

Furthermore, an interesting direction is to explore another type of amplification, which is the interchange of the tests between the original and evolved versions. In other words, I aim to co-evolve the tests of the original and evolved versions, respectively forward and backward to the evolved and original versions, while removing duplicates. 

Finally, the knowledge about the generated code elements from the metamodel elements (e.g., getter/setter for EAttribute, class/interface for EClass, etc.) is so far hard-coded in the implementation. The mappings must be provided for my approach to be able to trace the tests. I could express these mappings in a generic way as a configuration file taken as input in my approach. Thus, I could reuse my approach more easily in other scenarios than EMF Eclipse (e.g., jHipster), given their corresponding mappings as input. 
%This is left as future work to generalize our approach.

 

 

%%


%evaluate existing approaches of automatic code co-evolution with out technique to better assess their effect on the code. 

\subsection{An empirical study on leveraging LLMs for metamodels and code co-evolution}


First, I am interested in evaluating my approach in other technological spaces than EMF, such as OpenAPI and the challenge of the API evolution impact on clients' code. After that, I could transform my approach into a DSL-based approach with a graphical user interface for the output report instead of a CSV file. To facilitate prompt generation and enhance the option of prompt variation, a DSL would be a viable solution. An important consideration for future studies is to replicate my empirical study with other LLMs and other contexts of co-evolution (e.g., between code and test \cite{le2021untangling}). Another actionable element is to investigate the mining of contextual information from Software Engineering tasks to enrich the prompts, and then improve baseline results.

Unlike my automatic co-evolution approach detailed in Chapter~\ref{autocoevol}, the use of LLMs for the co-evolution of other artifacts in MDE ecosystem is still unexploited. Thus, I am initially curious about revisiting the empirical study on leveraging LLMs for metamodel co-evolution with models, transformations, and constraints. The future results of these empirical studies may guide me in providing practical tools for MDE community. 


Finally, since my contributions focus on empirically studying the use of \LLM~in metamodel and code co-evolution, I plan to implement an alternative of the quick fix engine in the Eclipse IDE based on the generated prompt structure. Integrating my prompt-based metamodel and code co-evolution in the IDE will have a direct impact in helping MDE developers and language engineers. 
