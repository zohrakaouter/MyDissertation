\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}
\chaptermark{Conclusion}

\section{ Summary of contributions and limitations}
%\subsection{Automated testing of metamodels and code co-evolution}
%\subsection{An Empirical Study on Leveraging LLMs for Metamodels and Code Co-evolution}
In this thesis, I started by proposing an automatic code co-evolution approach with evolving metamodel.
This approach effectively automates code co-evolution, resolving 100\% of errors, that we can divide into direct and indirect errors, and reaching 82\% of precision and 81\% of recall. The automation of the approach allows significantly reducing manual effort. The strategy prioritizes minimal deletions when possible, ensuring that necessary changes do not remove more code than required to apply the metamodel changes. Moreover, test execution results showed no significant behavioral differences between the original and co-evolved code, reinforcing the correctness of the automated approach in preserving code functionality. Additionally, the method is computationally efficient, with an average resolution time of less than half a second per error, making it scalable even on standard hardware. The comparison with IDE quick fixes highlights their limitations in handling co-evolution scenarios. Quick fixes omit contextually information of  metamodel changes, leading to applying different fixes than the expected developers' resolutions.
Finally, the comparison with the semi-automatic co-evolution approach \cite{Khelladi2020} highlights a key trade-off between automation and precision with a difference of almost the difference is only -10\%, knowing that the automatic approach logs the detailed co-evolution operations in a report that developers can check later. 

%limitations

%- I did not handle multiple, inter-dependent metamodels' evolution.
%- 
%- Evaluation does not cover of poThe size of the projects, number resulted compiling errors is among limitations.
%- I evaluated only on EMF data set only and no other technological space.ssible situations of metamodel changes and errors.
%-I use Evosuite generated tests only and not manually written tests.

While our approach demonstrates strong performance in automating code co-evolution, it has several limitations. First, it does not handle cases where multiple, interdependent metamodels evolve simultaneously, which can introduce additional complexity. Additionally, the evaluation is constrained by the size of the projects and the number of resulting compilation errors, limiting the generalizability of the results. Another limitation is the focus solely on the EMF dataset, without assessing its applicability in other technological spaces. Furthermore, the evaluation does not cover all possible metamodel changes and error scenarios, meaning certain edge cases may not be addressed. Lastly, our testing relies only on EvoSuite-generated tests, without incorporating manually written tests, that we argued its usefulness, but may not be complete in the context of out evaluation.



%My thorough evaluation proved that automatic co-evolution gives good results comparing to semi-automatic code co-evolution
Second, I developed an approach for automated testing of code co-evolution. 
Before tackling the evaluation of the approach itself, I conducted a user study to highlight the challenges of manually tracing impacted tests after metamodel evolution which can be both incomplete and error-prone. Since the approach automated testing of code co-evolution is based on tracing the impacted set of tests before and after co-evolution, I had to confirm that the approach successfully traces impacted tests due to metamodel changes, isolating the relevant tests. At this point of the evaluation, I included projects with both manually written and automatically generated tests. The correctness of the co-evolution is indicated by the fluctuation in the number of passing, failing, or erroneous tests. Overall, I found that running the traced subset of tests can indicate if the co-evolution was processed correctly or not, regardless of whether they tests are manually or automatically written, thus gaining effort and time.

%Limitation
%- The size of the projects, number resulted compiling errors is among limitations.
%- I evaluated only on EMF data set only and no other technological space.
%- The visual output of our tool is still basic, and has to be improved in order to assist developers in the code co-evolution process.
While our approach shows promising results, it has several limitations. The size of the projects and the number of both manually written and generated tests present challenges, as larger projects with extensive test suites may require additional optimization for efficient handling. Additionally, our evaluation was conducted only on the EMF dataset, limiting its applicability to other technological spaces. Future work should explore its effectiveness in different contexts. Moreover, the visual output of our tool remains basic and needs improvement to better assist developers in the code co-evolution process by providing clearer insights and more intuitive interactions.

Finally, with the emergence of Large Language Models, I Investigated in my empirical study the ability of Chatgpt to give correct co-evolutions. Setting the hyperparameter temperature to 0.2, I obtained  88.7\% in handling code co-evolution, improving to 95.2\% for complex metamodel changes, thus highlighting ChatGPT’s promising potential. Then, I analyzed the impact of varying the temperature on the results. I found that lower temperature of 0 and 0.2 give better coevolutions. Interestingly, we obtained the same results over 5 different runs, which suggests the efficiency our
prompt structure in narrowing the scope of possibilities of ChatGPT’s answers. Furthermore, I studied the impact of varying the prompt structure of the prompts on the quality of Chatgpt proposed co-evolutions. Providing only the code instruction containing the error yielded the best results, in some cases, as it helped ChatGPT focus on the specific issue. Asking for alternative answers also improved accuracy but to a lesser extent. However, changing the order by describing the metamodel change first instead of the describing the abstraction gap between the metamodel impacted element and the erroneous code led to a drop in correctness, as ChatGPT sometimes assumed deleted elements still existed. Thus,showing that well-structured prompts significantly enhance ChatGPT’s performance. Finally, the comparison of the proposed co-evolutions with quick fixes revealed significant limitations in the latter. The lack of context-awareness in IDE quick fixes and understanding of the abstraction gap made them unsuitable for the co-evolution tasks. In contrast, our prompt generation approach addresses these gaps by considering the impacted code’s context and the metamodel changes, guided Chatgpt to propose more accurate co-evolutions.

%Limitations 
%- The empirical study was based only gpt-3.5, no other large language models was used to get more confidence on the results.
%- The LLM hyper parameters: top\_p was fixed, the temperature was variated. Studying the variation of all hyperparameters may give more insights on the ability of the model in proposing co-evolution
%- The empirical study was base don a prototype, the is not a yet a tool to assist developers in code co-evolution, that is based on our approach.
Regarding limitations of this study include the fact that the empirical evaluation was conducted solely on GPT-3.5, without testing other large language models, which could provide greater confidence in the results. Additionally, among the LLM hyperparameters, only the temperature was varied while top\_p remained fixed; a more comprehensive analysis involving the variation of all hyperparameters could offer deeper insights into the model's capability in proposing co-evolutions. Furthermore, the study was based on a prototype implementation, and there is not yet a fully developed tool to assist developers in code co-evolution based on our approach.
\section{Perspectives}
\subsection{Automated co-evolution of metamodels and code}
We first plan to explore ordering the errors in the code before co-evolving them. In fact, we co-evolve the errors in the same order they are retrieved by the JDT. Thus, we will investigate whether it could reach better correctness with faster co-evolution or not. 

We also plan to evaluate our approach on more case studies and on the case of simultaneous multiple metamodel evolution. 
%
Finally, we plan to extend our resolutions with a replace resolution, based on distance metrics to find element replacements to co-evolve the code. After that, we can rely on our approach to extend it with a search-based heuristics, such as genetic algorithms to explore different paths of co-evolution, in contrast to a single path that we compute in this paper. In particular, to explore the different alternative resolutions of $CR1-CR4$. 

%
%a\kaouter{}
%\DK{either we put it as future work, maybe even in discussion above 4.5, and then refer to it here and say future work.}
%
%Finally, we will evaluate our approach on more case studies and on the case of simultaneous multiple metamodel evolution. 

%\newpage

\subsection{Automated testing of metamodels and code co-evolution}
First, we plan to improve the performance of our implementation with optimization of the tests' tracing.  %first evaluate on more case studies that could have manually written test. 
%we will 
We also plan to extend our approach to projects that use an equivalent form of metamodels in other technological space than Eclipse, such as JHipster and OpenAPI that both generate code from a model specification similar to a metamodel. Thus, we can have alternative case studies. 

After that, we plan to investigate the techniques of test amplification on the selected tests we traced from the metamodel changes. Indeed, once we select a subset of tests, we could amplify them by generating more similar tests, yet, with different assertions to cover more corner cases. This would amplify the behavioral check of the code co-evolution. 

Moreover, the knowledge about the generated code elements from the metamodel elements (e.g., getter/setter for EAttribute, class/interface for EClass, etc.) is so far hard-coded in the implementation. The mappings must be provided for our approach to be able to trace the tests. We could express these mappings in a generic way as a configuration file taken as input in our approach. Thus, we could reuse our approach more easily in other scenarios than EMF Eclipse (e.g., jHipster), given their corresponding mappings as input. This is left as future work to generalize our approach.

Finally, we will also explore another type of amplification, which is the interchange of the tests between the original and evolved versions. In other words, we aim to co-evolve the tests of the original and evolved versions, respectively forward and backward to the evolved and original versions, while removing duplicates. 

%%


%evaluate existing approaches of automatic code co-evolution with out technique to better assess their effect on the code. 

\subsection{An Empirical Study on Leveraging LLMs for Metamodels and Code Co-evolution}


First, we intend to evaluate or approach in other technological spaces than EMF, such as OpenAPI and the challenge of the API evolution impact on clients' code. After that, we plan to transform our approach into a DSL-based approach with a graphical user interface for the output report instead of a CSV file. To facilitate prompt generation and enhance the option of prompt variation, a DSL would be a viable solution. We also plan to replicate our empirical study with other LLMs and other contexts of co-evolution (e.g., between code and test \cite{le2021untangling}).  Another actionable element is to investigate the mining of contextual information from Software Engineering tasks to enrich the prompts, and then improve baseline results.

Finally, since our contributions focus on empirically studying the use of \LLM in metamodel and code co-evolution, we plan to implement an alternative of the quick fix engine in the Eclipse IDE based on our generated prompt structure. Integrating our prompt-based metamodel and code co-evolution in the IDE will have a direct impact in helping MDE developers and language engineers. 
